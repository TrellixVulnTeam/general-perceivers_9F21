# Distributed

The distributed computation technology is what has enabled the deep learning revolution to really come to the forefront. In this folder I am exploring how paralelisms (model and data) can be implemented right into `gperc`. For this I am going to be reading from `pytorch` documentation (if possible will extend to `deepspeed`). Here is the index for code file added yet:

1. `torch_pipe_dist`: This runs [this](https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html) tutorial
